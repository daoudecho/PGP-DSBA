---
title: "Untitled"
output: html_document
---

#===============================================================================================================================================
#$$$$$$$$$$$$$$$$$$$$        PART 1   $$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$
#===============================================================================================================================================

packages:
```{r}
library(readr)
library(base)
library(tidyverse)
library(psych)
library(dlookr)
library(DataExplorer)
library(caret)
library(Hmisc)
library(corrplot)
library(InformationValue)
library(xgboost)
library(randomForest)
library(rpart)
library(rattle)
library(varImp)
library(ROCR)
library(ineq)
library(rpart.plot)
```

loeading data:
```{r}
Survey_data <- read_csv("C:/Users/daoud/Downloads/PGP DSBA/Capstone Project/Marketing Project-Survey data.csv")

Flight_data <- read_csv("C:/Users/daoud/Downloads/PGP DSBA/Capstone Project/Marketing Project-Flight data.csv")
```

Merging two data frame together by CustomerID : 
```{r}
Total_data<-merge(x = Flight_data, y = Survey_data, by.x = "CustomerID", by.y = "CustomerId")
rm(Flight_data) # remove Flight_data
rm(Survey_data) # remove Survey_data
head(Total_data,10)
```

Variable Transformation:
```{r}
#Gender
Total_data$Gender<-       as.numeric(ifelse(Total_data$Gender=='Female',0,1))
#CustomerType
Total_data$CustomerType<- as.numeric(ifelse(Total_data$CustomerType=='disloyal Customer',0,1))
#TypeTravel
Total_data$TypeTravel<-   as.numeric(ifelse(Total_data$TypeTravel=='Personal Travel',0,1))
#Class
Total_data$Class<-        as.numeric(ifelse(Total_data$Class=='Eco',0,ifelse(Total_data$Class=='Eco Plus',1,2)))
#Satisfaction
Total_data$Satisfaction <-as.numeric(ifelse(Total_data$Satisfaction=='satisfied',1,0))
#seat_comfort
Total_data$Seat_comfort=factor(Total_data$Seat_comfort,levels=c("extremely poor","poor",
          "need improvement","acceptable","good","excellent"),labels=c(0,1,2,3,4,5),ordered = T)
Total_data$Seat_comfort =  as.numeric(as.character(Total_data$Seat_comfort))

#Departure.Arrival.time_convenient
Total_data$Departure.Arrival.time_convenient=factor(Total_data$Departure.Arrival.time_convenient,levels=c("extremely poor","poor",
          "need improvement","acceptable","good","excellent"),labels=c(0,1,2,3,4,5),ordered = T)
Total_data$Departure.Arrival.time_convenient =  as.numeric(as.character(Total_data$Departure.Arrival.time_convenient))

#Food_drink
Total_data$Food_drink=factor(Total_data$Food_drink,levels=c("extremely poor","poor",
          "need improvement","acceptable","good","excellent"),labels=c(0,1,2,3,4,5),ordered = T)
Total_data$Food_drink =  as.numeric(as.character(Total_data$Food_drink))

#Gate_location
Total_data$Gate_location=factor(Total_data$Gate_location,levels=c("very inconvinient","Inconvinient","need improvement","manageable","Convinient","very convinient" ),labels=c(0,1,2,3,4,5),ordered = T)
Total_data$Gate_location =  as.numeric(as.character(Total_data$Gate_location))


#Inflightwifi_service
Total_data$Inflightwifi_service=factor(Total_data$Inflightwifi_service,levels=c("extremely poor","poor",
          "need improvement","acceptable","good","excellent"),labels=c(0,1,2,3,4,5),ordered = T)
Total_data$Inflightwifi_service =  as.numeric(as.character(Total_data$Inflightwifi_service))

#Inflight_entertainment
Total_data$Inflight_entertainment=factor(Total_data$Inflight_entertainment,levels=c("extremely poor","poor",
          "need improvement","acceptable","good","excellent"),labels=c(0,1,2,3,4,5),ordered = T)
Total_data$Inflight_entertainment =  as.numeric(as.character(Total_data$Inflight_entertainment))

#Online_support 
Total_data$Online_support =factor(Total_data$Online_support ,levels=c("extremely poor","poor",
          "need improvement","acceptable","good","excellent"),labels=c(0,1,2,3,4,5),ordered = T)
Total_data$Online_support  =  as.numeric(as.character(Total_data$Online_support))

#Ease_of_Onlinebooking
Total_data$Ease_of_Onlinebooking=factor(Total_data$Ease_of_Onlinebooking,levels=c("extremely poor","poor",
          "need improvement","acceptable","good","excellent"),labels=c(0,1,2,3,4,5),ordered = T)
Total_data$Ease_of_Onlinebooking =  as.numeric(as.character(Total_data$Ease_of_Onlinebooking))

#Onboard_service
Total_data$Onboard_service=factor(Total_data$Onboard_service,levels=c("extremely poor","poor",
          "need improvement","acceptable","good","excellent"),labels=c(0,1,2,3,4,5),ordered = T)
Total_data$Onboard_service =  as.numeric(as.character(Total_data$Onboard_service))

#Leg_room_service
Total_data$Leg_room_service=factor(Total_data$Leg_room_service,levels=c("extremely poor","poor",
          "need improvement","acceptable","good","excellent"),labels=c(0,1,2,3,4,5),ordered = T)
Total_data$Leg_room_service =  as.numeric(as.character(Total_data$Leg_room_service))

#Baggage_handling
Total_data$Baggage_handling=factor(Total_data$Baggage_handling,levels=c("extremely poor","poor",
          "need improvement","acceptable","good","excellent"),labels=c(0,1,2,3,4,5),ordered = T)
Total_data$Baggage_handling =  as.numeric(as.character(Total_data$Baggage_handling))

#Checkin_service
Total_data$Checkin_service=factor(Total_data$Checkin_service,levels=c("extremely poor","poor",
          "need improvement","acceptable","good","excellent"),labels=c(0,1,2,3,4,5),ordered = T)
Total_data$Checkin_service =  as.numeric(as.character(Total_data$Checkin_service))

#Cleanliness
Total_data$Cleanliness=factor(Total_data$Cleanliness,levels=c("extremely poor","poor",
          "need improvement","acceptable","good","excellent"),labels=c(0,1,2,3,4,5),ordered = T)
Total_data$Cleanliness =  as.numeric(as.character(Total_data$Cleanliness))

#Online_boarding
Total_data$Online_boarding=factor(Total_data$Online_boarding,levels=c("extremely poor","poor",
          "need improvement","acceptable","good","excellent"),labels=c(0,1,2,3,4,5),ordered = T)
Total_data$Online_boarding =  as.numeric(as.character(Total_data$Online_boarding))

```

rename variable:
```{r}
Total_data<-rename(Total_data,
                   Dep_del_M=DepartureDelayin_Mins,
                   Arr_Del_M=ArrivalDelayin_Mins,
                   Time_cov=Departure.Arrival.time_convenient,
                   wifi_ser=Inflightwifi_service,
                   entertainment=Inflight_entertainment,
                   online_booking=Ease_of_Onlinebooking,
                   Onboard_ser=Onboard_service,
                   Leg_room_ser=Leg_room_service,
                   Bag_handl=Baggage_handling,
                   Checkin_ser=Checkin_service)
```

Calculating descriptive statistics:
```{r}
describe(Total_data)
```

Visualization of the correlation matrix:
```{r}
plot_correlate(Total_data)
```

ploting Missing Values:
```{r}
plot_missing(Total_data)
```

#===============================================================================================================================================
#$$$$$$$$$$$$$$$$$$$$        PART 2    $$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$
#===============================================================================================================================================

Missing Value Treatment: replace NA for [CustomerType,TypeTravel] with 'median' and [Arr_Del_M] with 'mean' :
```{r}
library(imputeTS)
Total_data$CustomerType <-na.mean(Total_data$CustomerType,option = 'median') # mediam for binary variable
Total_data$TypeTravel   <-na.mean(Total_data$TypeTravel,option = 'median')   # mediam for binary variable
Total_data$Arr_Del_M    <-na.mean(Total_data$Arr_Del_M,option = 'mean')      # mean for continuous variable
```

replace NA for [Time_cov,Food_drink,Onboard_ser] with 6 :  
```{r}
Total_data$Time_cov<-Total_data$Time_cov %>% replace_na(6)
Total_data$Food_drink<-Total_data$Food_drink %>% replace_na(6)
Total_data$Onboard_ser<-Total_data$Onboard_ser %>% replace_na(6)

summary(Total_data)
```


Outlier treatment:
```{r}
library(ggplot2)
# Age :
quantile(Total_data$Age,c(0.01,0.02,0.03,0.1,0.2,0.3,0.4,0.50,0.6,0.7,0.8,0.9,0.95,0.99,1))
#  no indicates the existance of outlier observations in this variable
plot(density(Total_data$Age),main="Age")
# Density plot looks normal-skewed distribution 


# Flight_Distance :
quantile(Total_data$Flight_Distance,c(0.01,0.02,0.03,0.1,0.2,0.3,0.4,0.50,0.6,0.7,0.8,0.9,0.95,0.99,1))
#  it indicates the existance of a few outlier observations in this variable
plot(density(Total_data$Flight_Distance),main="Flight_Distance")
# Density plot looks right-skewed 

# Dep_del_M :
quantile(Total_data$Dep_del_M,c(0.01,0.02,0.03,0.1,0.2,0.3,0.4,0.50,0.6,0.7,0.8,0.9,0.95,0.99,1))
#  it indicates the existance of  outlier observations in this variable
plot(density(Total_data$Dep_del_M),main="Dep_del_M")
# Density plot looks extremely right-skewed 
# we will compare 'Dep_del_M' with 'Arr_Del_M'  

#Arr_Del_M:
quantile(Total_data$Arr_Del_M,c(0.01,0.02,0.03,0.1,0.2,0.3,0.4,0.50,0.6,0.7,0.8,0.9,0.95,0.99,1))
#  it indicates the existance of  outlier observations in this variable
plot(density(Total_data$Arr_Del_M),main="Arr_Del_M")
# Density plot looks extremely right-skewed
# almost identical, it is Natural for 'Dep_del_M' and 'Arr_Del_M'

head(Total_data,10)
```

Exploratory Data Analysis:

correlation:
```{r}
cor1<-cor(x=Total_data,y = NULL, method = "pearson")

cor_5 <- rcorr(as.matrix(Total_data))
M <- cor_5$r
p_mat <- cor_5$P

corrplot(cor1, type = "lower", method = "number",
         p.mat = p_mat, sig.level = 0.01, insig = "blank")# with significance level
```

# rpart: Single decision tree : 
```{r}
rpart_model <- train( Satisfaction~ ., data = d_train,
                     method = "rpart",
                     minbucket = 10,
                     cp = 0,
                     tuneLength = 10)

rpart_model

varimp<-varImp(rpart_model)
print(varimp)

fancyRpartPlot(rpart_model$finalModel)
```


```{r}
regressor <- randomForest(Satisfaction~ ., data = Total_data, importance=TRUE) # fit the random forest with default parameter

varImp(regressor) # get variable importance, based on mean decrease in accuracy

varImp(regressor, conditional=TRUE) # conditional=True, adjusts for correlations between predictors

varimpAUC(regressor) # more robust towards class imbalance.
```

Visualizations:
# visualization:
```{r}
plot_histogram(Total_data)
```

Satisfaction VS Grender:
```{r}
par(mfrow=c(1,2))
hist(Total_data$Gender[Total_data$Satisfaction==1],col = "blue",xlab = "Gender_Male",main = "Histogram Satisfaction",ylim = c(0,35000))
hist(Total_data$Gender[Total_data$Satisfaction==0],col = "blue",xlab = "Gender_Female",main = "Histogram Satisfaction",ylim = c(0,35000))
```

Satisfaction VS CustomerType:
```{r}
par(mfrow=c(1,2))
hist(Total_data$CustomerType[Total_data$Satisfaction==1],col = "blue",xlab = "Disloyal Customer ",main = "Histogram Satisfaction",ylim = c(0,50000))
hist(Total_data$CustomerType[Total_data$Satisfaction==0],col = "blue",xlab = "Loyal customer ",main = "Histogram Satisfaction",ylim = c(0,50000))
```
Satisfaction VS class:
```{r}
par(mfrow=c(1,3))
hist(Total_data$Satisfaction[Total_data$Class==1],col = "blue",xlab = "Eco_Plus_class ",main = "Histogram Satisfaction",ylim = c(0,35000))
hist(Total_data$Satisfaction[Total_data$Class==0],col = "blue",xlab = "Eco_class ",main = "Histogram Satisfaction",ylim = c(0,35000))
hist(Total_data$Satisfaction[Total_data$Class==2],col = "blue",xlab = "Business_class ",main = "Histogram Satisfaction",ylim = c(0,35000))

```
Satisfaction VS TypeTravel:
```{r}
par(mfrow=c(1,2))
hist(Total_data$TypeTravel[Total_data$Satisfaction==1],col = "blue",xlab = "Business travel ",main = "Histogram Satisfaction",ylim = c(0,40000))
hist(Total_data$TypeTravel[Total_data$Satisfaction==0],col = "blue",xlab = "Personal Travel ",main = "Histogram Satisfaction",ylim = c(0,40000))
```

Data split into test and train:
```{r}
library(caTools)
Total_data$Satisfaction <- as.factor(ifelse(Total_data$Satisfaction,"Yes","No"))
set.seed(123)
sample = sample.split(Total_data,SplitRatio = 0.7) # 70% train data , 30% test data
d_train = subset(Total_data,sample == TRUE)
d_test = subset(Total_data,sample == FALSE)
print(paste0("train: ", nrow(d_train)))
print(paste0("test: ", nrow(d_test)))
# the data split is equal between Train and Test with original dataset.
prop.table(table(Total_data$Satisfaction)) 
prop.table(table(d_train$Satisfaction))
prop.table(table(d_test$Satisfaction))
```
#===============================================================================================================================================
#$$$$$$$$$$$$$$$$$$$$        PART 3    $$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$
#===============================================================================================================================================

setting general paramater for training :
```{r}
Ctrl <- trainControl(
              method = 'repeatedcv',           # repeatedcv : repeated random sub-sampling validation
              number = 10,                     # number of  k
              repeats = 3,                     # repeated k-fold cross-validation
              allowParallel = TRUE,
              classProbs = TRUE,              # Estimate class probabilities
              summaryFunction=twoClassSummary # should class probabilities be returned
    ) 
```

---------------------------  rpart MODEL  --------------------------------------------------------------------------
Train a single decision tree:
-------------------------------
```{r}
set.seed(2020)
rpart_model <- train(Satisfaction~ Gender+CustomerType +Age+TypeTravel+Class+Flight_Distance+Dep_del_M+Arr_Del_M+Seat_comfort+Time_cov+Food_drink+Gate_location+wifi_ser+entertainment+Online_support+online_booking+Onboard_ser+Leg_room_ser+Bag_handl+Checkin_ser+Cleanliness+Online_boarding,
                     data =d_train,
                     method = "rpart",
                     minbucket = 10,
                     cp = 0.01,
                     tuneLength = 10,
                     trControl = Ctrl)

rpart_model

```
## Plot the CP values and Tree:
```{r}
plot(rpart_model)
print(rpart_model$finalModel)
fancyRpartPlot(rpart_model$finalModel,cex=0.5, tweak=1)
rpart.plot::prp(rpart_model$finalModel, box.palette = "auto",branch.type = 5, yesno = FALSE, faclen = 0)
```
## Predict both class and probabilities:
```{r}
rpart_predict_test_prob <- predict(rpart_model, newdata = d_test, type = "prob")
rpart_predict_test_class <- predict(rpart_model, newdata = d_test, type = "raw")
## The Confusion Matrix :
caret::confusionMatrix(rpart_predict_test_class, d_test$Satisfaction, positive = "Yes")
```

## Concordance - Discordance (overall : rarely used, specific to certain domains)
```{r}
levels(d_test$Satisfaction) <- c("0", "1")
Concordance(actuals = d_test$Satisfaction, predictedScores = rpart_predict_test_prob[,2])
```

# ROC & Precision Recall Curves:
```{r}
# Creating the prediction object using ROCR library
levels(d_test$Satisfaction) <- c("No", "Yes")
pred_obj_rpart = prediction(rpart_predict_test_prob[,2], d_test$Satisfaction)

# ROC curve
ROC_curve = performance(pred_obj_rpart, "tpr", "fpr")
plot(ROC_curve, main = "ROC curve")
abline(a=0, b= 1, lty = 3)

# Precision recall curve
precision_recall_rpart <- performance(pred_obj_rpart, "ppv", "tpr")
plot(precision_recall_rpart, main = "Precisoin Recall curve")
abline(a=0, b= 1, lty = 3)

# Computing the area under the curve
auc = performance(pred_obj_rpart,"auc"); 
auc = as.numeric(auc@y.values)
auc

# Computing Gini
gini = ineq(rpart_predict_test_prob[,2], type="Gini")
gini

```

## Gain & Lift Chart:
```{r}
lift_rpart <- lift(d_test$Satisfaction ~ rpart_predict_test_prob[,2], data = d_test, class ="Yes")
ggplot(lift_rpart, plot = "lift")+ ggtitle("Lift Chart")
ggplot(lift_rpart, plot = "gain", valueitle("Gain Chart"))
```

## KS table & KS plot:
```{r}
ks_stat(d_test$Satisfaction, rpart_predict_test_prob[,2]) # print KS

ks_stat(d_test$Satisfaction, rpart_predict_test_prob[,2], returnKSTable = T) # print KS table

ks_plot(d_test$Satisfaction, rpart_predict_test_prob[,2]) # plot KS 
```
---------------------------  Random Forest MODEL  ------------------------------------------------------------------------
## train Random Forest :
================================
```{r}
set.seed(2020)
rf_model <- train(Satisfaction ~ ., data = d_train,
                     method = "rf",
                     ntree = 21,   # number of tree
                     maxdepth = 8,
                     tuneLength = 8,
                     trControl = Ctrl)
rf_model
```
## Plot ROC and print the random forest:
```{r}
plot(rf_model)
print(rf_model$finalModel)
randomForest::importance(rf_model$finalModel)
randomForest::varImpPlot(rf_model$finalModel)
```
## Predict both class and probabilities: 
```{r}
rf_predict_test_prob <- predict(rf_model, newdata = d_test, type = "prob")
rf_predict_test_class <- predict(rf_model, newdata = d_test, type = "raw")
## The Confusion Matrix (most common way of evaluating a model)
caret::confusionMatrix(rf_predict_test_class, d_test$Satisfaction, positive = "Yes")
```

## Concordance - Discordance (overall : rarely used, specific to certain domains)
```{r}
levels(d_test$Satisfaction) <- c("0", "1")
Concordance(actuals = d_test$Satisfaction, predictedScores = rf_predict_test_prob[,2])
```
## ROC & Precision Recall Curves:
```{r}
# Creating the prediction object using ROCR library
levels(d_test$Satisfaction) <- c("No", "Yes")
pred_obj_rf = prediction(rf_predict_test_prob[,2], d_test$Satisfaction)


# ROC curve
ROC_curve = performance(pred_obj_rf, "tpr", "fpr")
plot(ROC_curve, main = "ROC curve")
abline(a=0, b= 1, lty = 3)


# Precision recall curve
precision_recall_rf <- performance(pred_obj_rf, "ppv", "tpr")
plot(precision_recall_rf, main = "Precisoin Recall curve")
abline(a=0, b= 1, lty = 3)

# Computing the area under the curve
auc = performance(pred_obj_rf,"auc"); 
auc = as.numeric(auc@y.values)
auc

# Computing Gini
gini = ineq(rf_predict_test_prob[,2], type="Gini")
gini

```
## Gain & Lift Chart:
```{r}
lift_rf <- lift(d_test$Satisfaction ~ rf_predict_test_prob[,2], data = d_test, class ="Yes")
ggplot(lift_rf, plot = "lift")+ ggtitle("Lift Chart")
ggplot(lift_rf, plot = "gain") + ggtitle("Gain Chart")
```
## KS table & KS plot:
```{r}
ks_stat(d_test$Satisfaction, rf_predict_test_prob[,2]) # print KS

ks_stat(d_test$Satisfaction, rf_predict_test_prob[,2], returnKSTable = T) # print table KS

ks_plot(d_test$Satisfaction, rf_predict_test_prob[,2]) # plot KS
```
--------------------------- KNN MODEL  -----------------------------------------------------------------------------
# KNN:
================================
```{r}
set.seed(2020)
knn_model<-train(
  Satisfaction~.,
  data = d_train,
  method="knn",
  tuneLength = 3,
  trControl = Ctrl)
knn_model
```
## Plot ROC and print the Knn:
```{r}
plot(knn_model)
print(knn_model$finalModel)
```
## Predict both class and probabilities
```{r}
knn_predict_test_prob <- predict(knn_model, newdata = d_test, type = "prob")
knn_predict_test_class <- predict(knn_model, newdata = d_test, type = "raw")
## The Confusion Matrix (most common way of evaluating a model)
caret::confusionMatrix(knn_predict_test_class, d_test$Satisfaction, positive = "Yes")
```
## Concordance - Discordance (overall : rarely used, specific to certain domains)
```{r}
levels(d_test$Satisfaction) <- c("0", "1")
Concordance(actuals = d_test$Satisfaction, predictedScores = knn_predict_test_prob[,2])
```

## ROC & Precision Recall Curves:
```{r}
# Creating the prediction object using ROCR library
levels(d_test$Satisfaction) <- c("No", "Yes")
pred_obj_knn = prediction(knn_predict_test_prob[,2], d_test$Satisfaction)


# ROC curve
ROC_curve = performance(pred_obj_knn, "tpr", "fpr")
plot(ROC_curve, main = "ROC curve")
abline(a=0, b= 1, lty = 3)


# Precision recall curve
precision_recall_knn <- performance(pred_obj_knn, "ppv", "tpr")
plot(precision_recall_knn, main = "Precisoin Recall curve")
abline(a=0, b= 1, lty = 3)

# Computing the area under the curve
auc = performance(pred_obj_knn,"auc"); 
auc = as.numeric(auc@y.values)
auc

# Computing Gini
gini = ineq(knn_predict_test_prob[,2], type="Gini")
gini

```
## Gain & Lift Chart:
```{r}
lift_knn <- lift(d_test$Satisfaction ~ knn_predict_test_prob[,2], data = d_test, class ="Yes")
ggplot(lift_knn, plot = "lift")+ ggtitle("Lift Chart")
ggplot(lift_knn, plot = "gain", valueitle("Gain Chart"))
```
## KS table & KS plot:
```{r}
ks_stat(d_test$Satisfaction, knn_predict_test_prob[,2]) # print KS

ks_stat(d_test$Satisfaction, knn_predict_test_prob[,2], returnKSTable = T) # print table KS

ks_plot(d_test$Satisfaction, knn_predict_test_prob[,2]) # plot KS
```

---------------------------  naive bayes MODEL  --------------------------------------------------------------
# model naive base:
================================
```{r}
set.seed(2020)
naive_model<-train(
  Satisfaction~.,
  data = d_train,
  method="naive_bayes",
  trControl = Ctrl)
naive_model
```

## Plot ROC and print the naive bayes:
```{r}
plot(naive_model)
print(naive_model$finalModel)
```
## Predict both class and probabilities:
```{r}
naive_predict_test_prob <- predict(naive_model, newdata = d_test, type = "prob")
naive_predict_test_class <- predict(naive_model, newdata = d_test, type = "raw")
## The Confusion Matrix (most common way of evaluating a model)
caret::confusionMatrix(naive_predict_test_class, d_test$Satisfaction, positive = "Yes")
```
## Concordance - Discordance (overall : rarely used, specific to certain domains)
```{r}
levels(d_test$Satisfaction) <- c("0", "1")
Concordance(actuals = d_test$Satisfaction, predictedScores = naive_predict_test_prob[,2])
```

## ROC & Precision Recall Curves:
```{r}
# Creating the prediction object using ROCR library
levels(d_test$Satisfaction) <- c("No", "Yes")
pred_obj_naive = prediction(naive_predict_test_prob[,2], d_test$Satisfaction)


# ROC curve
ROC_curve = performance(pred_obj_naive, "tpr", "fpr")
plot(ROC_curve, main = "ROC curve")
abline(a=0, b= 1, lty = 3)


# Precision recall curve
precision_recall_naive <- performance(pred_obj_naive, "ppv", "tpr")
plot(precision_recall_naive, main = "Precisoin Recall curve")
abline(a=0, b= 1, lty = 3)

# Computing the area under the curve
auc = performance(pred_obj_naive,"auc"); 
auc = as.numeric(auc@y.values)
auc

# Computing Gini
gini = ineq(naive_predict_test_prob[,2], type="Gini")
gini

```
## Gain & Lift Chart
```{r}
lift_naive <- lift(d_test$Satisfaction ~ naive_predict_test_prob[,2], data = d_test, class ="Yes")
ggplot(lift_naive, plot = "lift")+ ggtitle("Lift Chart")
ggplot(lift_naive, plot = "gain", valueitle("Gain Chart"))
```
## KS table & KS plot:
```{r}
ks_stat(d_test$Satisfaction, naive_predict_test_prob[,2]) # print KS

ks_stat(d_test$Satisfaction, naive_predict_test_prob[,2], returnKSTable = T) # print table KS

ks_plot(d_test$Satisfaction, naive_predict_test_prob[,2]) # plot KS
```
---------------------------  Logistic Regression MODEL  ----------------------------------------
# Logistic Regression :
================================
```{r}
set.seed(2020)
glm_model<-train(
  Satisfaction~.,
  data = d_train,
  method = "glm",
  family = "binomial",
  trControl = Ctrl
)
glm_model
```

##print the glm:
```{r}
print(glm_model$finalModel)
```
## Predict both class and probabilities:
```{r}
glm_predict_test_prob <- predict(glm_model, newdata = d_test, type = "prob")
glm_predict_test_class <- predict(glm_model, newdata = d_test, type = "raw")
## The Confusion Matrix (most common way of evaluating a model)
caret::confusionMatrix(glm_predict_test_class, d_test$Satisfaction, positive = "Yes")
```
## Concordance - Discordance (overall : rarely used, specific to certain domains)
```{r}
levels(d_test$Satisfaction) <- c("0", "1")
Concordance(actuals = d_test$Satisfaction, predictedScores = glm_predict_test_prob[,2])
```

## ROC & Precision Recall Curves:
```{r}
# Creating the prediction object using ROCR library
levels(d_test$Satisfaction) <- c("No", "Yes")
pred_obj_glm = prediction(glm_predict_test_prob[,2], d_test$Satisfaction)


# ROC curve
ROC_curve = performance(pred_obj_glm, "tpr", "fpr")
plot(ROC_curve, main = "ROC curve")
abline(a=0, b= 1, lty = 3)


# Precision recall curve
precision_recall_glm <- performance(pred_obj_glm, "ppv", "tpr")
plot(precision_recall_glm, main = "Precisoin Recall curve")
abline(a=0, b= 1, lty = 3)

# Computing the area under the curve
auc = performance(pred_obj_glm,"auc"); 
auc = as.numeric(auc@y.values)
auc

# Computing Gini
gini = ineq(glm_predict_test_prob[,2], type="Gini")
gini

```
## Gain & Lift Chart:
```{r}
lift_glm <- lift(d_test$Satisfaction ~ glm_predict_test_prob[,2], data = d_test, class ="Yes")
ggplot(lift_glm, plot = "lift")+ ggtitle("Lift Chart")
ggplot(lift_glm, plot = "gain", valueitle("Gain Chart"))
```
## KS table & KS plot:
```{r}
ks_stat(d_test$Satisfaction, glm_predict_test_prob[,2]) # print KS

ks_stat(d_test$Satisfaction, glm_predict_test_prob[,2], returnKSTable = T) # print table KS

ks_plot(d_test$Satisfaction, glm_predict_test_prob[,2]) # plot KS
```
---------------------------  BAGGING MODEL  ------------------------------------------------------------------
# bagging :
================================
```{r}
set.seed(2020)
bagging_model<-train(
  Satisfaction~.,
  data = d_train,
  method = "treebag",
  nleaves=10,
  ntrees=5,
  trControl=Ctrl,
  importance=TRUE)
bagging_model
```

##  print the bagging:
```{r}
print(bagging_model$finalModel)
```
## Predict both class and probabilities:
```{r}
bagging_predict_test_prob <- predict(bagging_model, newdata = d_test, type = "prob")
bagging_predict_test_class <- predict(bagging_model, newdata = d_test, type = "raw")
## The Confusion Matrix (most common way of evaluating a model)
caret::confusionMatrix(bagging_predict_test_class, d_test$Satisfaction, positive = "Yes")
```
## Concordance - Discordance (overall : rarely used, specific to certain domains)
```{r}
levels(d_test$Satisfaction) <- c("0", "1")
Concordance(actuals = d_test$Satisfaction, predictedScores = bagging_predict_test_prob[,2])
```

## ROC & Precision Recall Curves:
```{r}
# Creating the prediction object using ROCR library
levels(d_test$Satisfaction) <- c("No", "Yes")
pred_obj_bagging = prediction(bagging_predict_test_prob[,2], d_test$Satisfaction)


# ROC curve
ROC_curve = performance(pred_obj_bagging, "tpr", "fpr")
plot(ROC_curve, main = "ROC curve")
abline(a=0, b= 1, lty = 3)


# Precision recall curve
precision_recall_bagging <- performance(pred_obj_bagging, "ppv", "tpr")
plot(precision_recall_bagging, main = "Precisoin Recall curve")
abline(a=0, b= 1, lty = 3)

# Computing the area under the curve
auc = performance(pred_obj_bagging,"auc"); 
auc = as.numeric(auc@y.values)
auc

# Computing Gini
gini = ineq(bagging_predict_test_prob[,2], type="Gini")
gini

```
## Gain & Lift Chart:
```{r}
lift_bagging <- lift(d_test$Satisfaction ~ bagging_predict_test_prob[,2], data = d_test, class ="Yes")
ggplot(lift_bagging, plot = "lift")+ ggtitle("Lift Chart")
ggplot(lift_bagging, plot = "gain", valueitle("Gain Chart"))
```
## KS table & KS plot:
```{r}
ks_stat(d_test$Satisfaction, bagging_predict_test_prob[,2]) # print KS

ks_stat(d_test$Satisfaction, bagging_predict_test_prob[,2], returnKSTable = T) # print table KS

ks_plot(d_test$Satisfaction, bagging_predict_test_prob[,2]) # plot KS
```
---------------------------  XGBOOST MODEL  -----------------------------------------------------------
# XGBoost: Xtreme Gradient boosting:
===================================
```{r}
set.seed(2020)
xgboost.grid <- expand.grid(nrounds = 150,
                            eta = c(0.01),
                            max_depth = c(4,7),
                            gamma = 0,               #default=0
                            colsample_bytree = 1,    #default=1
                            min_child_weight = 1,    #default=1
                            subsample = 1            #default=1
    )
xgboost_model <-train(Satisfaction~.,
                  data=d_train,
                  method="xgbTree",
                  trControl=Ctrl,
                  tuneGrid=xgboost.grid,
                  verbose=T,
                  nthread = 2
    )
xgboost_model
```
## Plot ROC and print the xgboost:
```{r}
plot(xgboost_model)
print(xgboost_model$finalModel)
```
## Predict both class and probabilities:
```{r}
xgboost_predict_test_prob <- predict(xgboost_model, newdata = d_test, type = "prob")
xgboost_predict_test_class <- predict(xgboost_model, newdata = d_test, type = "raw")
## The Confusion Matrix (most common way of evaluating a model)
caret::confusionMatrix(xgboost_predict_test_class, d_test$Satisfaction, positive = "Yes")
```
## Concordance - Discordance (overall : rarely used, specific to certain domains)
```{r}
levels(d_test$Satisfaction) <- c("0", "1")
Concordance(actuals = d_test$Satisfaction, predictedScores = xgboost_predict_test_prob[,2])
```

## ROC & Precision Recall Curves:
```{r}
# Creating the prediction object using ROCR library
levels(d_test$Satisfaction) <- c("No", "Yes")
pred_obj_xgboost = prediction(xgboost_predict_test_prob[,2], d_test$Satisfaction)


# ROC curve
ROC_curve = performance(pred_obj_xgboost, "tpr", "fpr")
plot(ROC_curve, main = "ROC curve")
abline(a=0, b= 1, lty = 3)


# Precision recall curve
precision_recall_xgboost <- performance(pred_obj_xgboost, "ppv", "tpr")
plot(precision_recall_xgboost, main = "Precisoin Recall curve")
abline(a=0, b= 1, lty = 3)

# Computing the area under the curve
auc = performance(pred_obj_xgboost,"auc"); 
auc = as.numeric(auc@y.values)
auc

# Computing Gini
gini = ineq(xgboost_predict_test_prob[,2], type="Gini")
gini

```

## Gain & Lift Chart:
```{r}
lift_xgboost <- lift(d_test$Satisfaction ~ xgboost_predict_test_prob[,2], data = d_test, class ="Yes")
ggplot(lift_xgboost, plot = "lift")+ ggtitle("Lift Chart")
ggplot(lift_xgboost, plot = "gain", valueitle("Gain Chart"))
```
## KS table & KS plot:
```{r}
ks_stat(d_test$Satisfaction, xgboost_predict_test_prob[,2]) # print KS

ks_stat(d_test$Satisfaction, xgboost_predict_test_prob[,2], returnKSTable = T) # print table KS

ks_plot(d_test$Satisfaction, xgboost_predict_test_prob[,2]) # plot KS
```
---------------------------  COMPARING MODELS  ---------------------------------------------
## COMPARING MODELS:
================================
```{r}
# Compare model performances using resample()
models_to_compare <- resamples(list(Logistic_Regression = glm_model, 
                                 Navie_Bayes = naive_model, 
                                 KNN = knn_model, 
                                 bagging = bagging_model,
                                 Single_tree = rpart_model,
                                 Random_Forest = rf_model, 
                                 Xgboost=xgboost_model
                                 ))

# Summary of the models performances
summary(models_to_compare)
```
## Draw box plots to compare models:
```{r}
scales <- list(x=list(relation="free"), y=list(relation="free"))
bwplot(models_to_compare, scales=scales)
```



